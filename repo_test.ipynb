{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.repository import Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gitrepo = \"open-mmlab/mmsegmentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = Repository(gitrepo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2149/2149 [01:09<00:00, 31.11it/s, file=tools/train.py]                                                                                                                                                                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built successfully.\n",
      "Traversed 2149 files. Found 169 collections.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "repo.build_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlockCollection ---------------------------------\n",
      "Title: docs/en/user_guides/4_train_test.md\n",
      "Blocks:\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: We provide tools/train.py to launch training jobs on a single GPU.\n",
      "The basic usage is as follows.\n",
      "\tCode:\n",
      "python tools/train.py  ${CONFIG_FILE} [optional arguments]\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: python\n",
      "\tDescription: If you would like to resume training from a specific checkpoint, you can use:\n",
      "\tCode:\n",
      "python tools/train.py ${CONFIG_FILE} --resume --cfg-options load_from=${CHECKPOINT}\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: Training on CPU: The process of training on the CPU is consistent with single GPU training if a machine does not have GPU. If it has GPUs but not wanting to use them, we just need to disable GPUs before the training process.\n",
      "\tCode:\n",
      "export CUDA_VISIBLE_DEVICES=-1\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: We provide tools/test.py to launch training jobs on a single GPU.\n",
      "The basic usage is as follows.\n",
      "\tCode:\n",
      "python tools/test.py ${CONFIG_FILE} ${CHECKPOINT_FILE} [optional arguments]\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: Testing on CPU: The process of testing on the CPU is consistent with single GPU testing if a machine does not have GPU. If it has GPUs but not wanting to use them, we just need to disable GPUs before the training process.\n",
      "\tCode:\n",
      "export CUDA_VISIBLE_DEVICES=-1\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: The basic usage is as follows:\n",
      "\tCode:\n",
      "sh tools/dist_train.sh ${CONFIG_FILE} ${GPU_NUM} [optional arguments]\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: An example:\n",
      "\tCode:\n",
      "# checkpoints and logs saved in WORK_DIR=work_dirs/pspnet_r50-d8_4xb4-80k_ade20k-512x512/\n",
      "# If work_dir is not set, it will be generated automatically.\n",
      "sh tools/dist_train.sh configs/pspnet/pspnet_r50-d8_4xb4-80k_ade20k-512x512.py 8 --work-dir work_dirs/pspnet_r50-d8_4xb4-80k_ade20k-512x512\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: Note: During training, checkpoints and logs are saved in the same folder structure as the config file under work_dirs/. A custom work directory is not recommended since evaluation scripts infer work directories from the config file name. If you want to save your weights somewhere else, please use a symlink, for example:\n",
      "\tCode:\n",
      "ln -s ${YOUR_WORK_DIRS} ${MMSEG}/work_dirs\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: We provide tools/dist_test.sh to launch testing on multiple GPUs.\n",
      "The basic usage is as follows.\n",
      "\tCode:\n",
      "sh tools/dist_test.sh ${CONFIG_FILE} ${CHECKPOINT_FILE} ${GPU_NUM} [optional arguments]\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: An example:\n",
      "\tCode:\n",
      "./tools/dist_test.sh configs/pspnet/pspnet_r50-d8_4xb2-40k_cityscapes-512x1024.py \\\n",
      "    checkpoints/pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth 4\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: If you launch multiple jobs on a single machine, e.g., 2 jobs of 4-GPU training on a machine with 8 GPUs, you need to specify different ports (29500 by default) for each job to avoid communication conflict. Otherwise, there will be an error message saying RuntimeError: Address already in use.\n",
      "If you use dist_train.sh to launch training jobs, you can set the port in commands with the environment variable PORT.\n",
      "\tCode:\n",
      "CUDA_VISIBLE_DEVICES=0,1,2,3 PORT=29500 sh tools/dist_train.sh ${CONFIG_FILE} 4\n",
      "CUDA_VISIBLE_DEVICES=4,5,6,7 PORT=29501 sh tools/dist_train.sh ${CONFIG_FILE} 4\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: If you launch with multiple machines simply connected with ethernet, you can simply run the following commands:\n",
      "On the first machine:\n",
      "\tCode:\n",
      "NNODES=2 NODE_RANK=0 PORT=${MASTER_PORT} MASTER_ADDR=${MASTER_ADDR} sh tools/dist_train.sh ${CONFIG_FILE} ${GPUS}\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: On the second machine:\n",
      "\tCode:\n",
      "NNODES=2 NODE_RANK=1 PORT=${MASTER_PORT} MASTER_ADDR=${MASTER_ADDR} sh tools/dist_train.sh ${CONFIG_FILE} ${GPUS}\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: The basic usage is as follows:\n",
      "\tCode:\n",
      "[GPUS=${GPUS}] sh tools/slurm_train.sh ${PARTITION} ${JOB_NAME} ${CONFIG_FILE} [optional arguments]\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: Below is an example of using 4 GPUs to train PSPNet on a Slurm partition named dev, and set the work-dir to some shared file systems.\n",
      "\tCode:\n",
      "GPUS=4 sh tools/slurm_train.sh dev pspnet configs/pspnet/pspnet_r50-d8_512x1024_40k_cityscapes.py --work-dir work_dir/pspnet\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: The basic usage is as follows:\n",
      "\tCode:\n",
      "[GPUS=${GPUS}] sh tools/slurm_test.sh ${PARTITION} ${JOB_NAME} ${CONFIG_FILE} ${CHECKPOINT_FILE} [optional arguments]\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: Set the port through --cfg-options. This is more recommended since it does not change the original configs.GPUS=4 GPUS_PER_NODE=4 sh tools/slurm_train.sh ${PARTITION} ${JOB_NAME} config1.py ${WORK_DIR} --cfg-options env_cfg.dist_cfg.port=29500\n",
      "GPUS=4 GPUS_PER_NODE=4 sh tools/slurm_train.sh ${PARTITION} ${JOB_NAME} config2.py ${WORK_DIR} --cfg-options env_cfg.dist_cfg.port=29501\n",
      "Modify the config files to set different communication ports.\n",
      "In config1.py:enf_cfg = dict(dist_cfg=dict(backend='nccl', port=29500))\n",
      "In config2.py:enf_cfg = dict(dist_cfg=dict(backend='nccl', port=29501))\n",
      "Then you can launch two jobs with config1.py and config2.py.CUDA_VISIBLE_DEVICES=0,1,2,3 GPUS=4 sh tools/slurm_train.sh ${PARTITION} ${JOB_NAME} config1.py ${WORK_DIR}\n",
      "CUDA_VISIBLE_DEVICES=4,5,6,7 GPUS=4 sh tools/slurm_train.sh ${PARTITION} ${JOB_NAME} config2.py ${WORK_DIR}\n",
      "Set the port in the command using the environment variable 'MASTER_PORT':\n",
      "\tCode:\n",
      "CUDA_VISIBLE_DEVICES=0,1,2,3 GPUS=4 MASTER_PORT=29500 sh tools/slurm_train.sh ${PARTITION} ${JOB_NAME} config1.py ${WORK_DIR}\n",
      "CUDA_VISIBLE_DEVICES=4,5,6,7 GPUS=4 MASTER_PORT=29501 sh tools/slurm_train.sh ${PARTITION} ${JOB_NAME} config2.py ${WORK_DIR}\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: When you want to save the results, you can use --out to specify the output directory.\n",
      "\tCode:\n",
      "python tools/test.py ${CONFIG_FILE} ${CHECKPOINT_FILE} --out ${OUTPUT_DIR}\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: Here is an example to save the predicted results from model fcn_r50-d8_4xb4-80k_ade20k-512x512 on ADE20k validatation dataset.\n",
      "\tCode:\n",
      "python tools/test.py configs/fcn/fcn_r50-d8_4xb4-80k_ade20k-512x512.py ckpt/fcn_r50-d8_512x512_80k_ade20k_20200614_144016-f8ac5082.pth --out work_dirs/format_results\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: python\n",
      "\tDescription: You also can modify the config file to define output_dir. We also take\n",
      "fcn_r50-d8_4xb4-80k_ade20k-512x512 as example just add\n",
      "test_evaluator in configs/fcn/fcn_r50-d8_4xb4-80k_ade20k-512x512.py\n",
      "\tCode:\n",
      "test_evaluator = dict(type='IoUMetric', iou_metrics=['mIoU'], output_dir='work_dirs/format_results')\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: then run command without --out:\n",
      "\tCode:\n",
      "python tools/test.py configs/fcn/fcn_r50-d8_4xb4-80k_ade20k-512x512.py ckpt/fcn_r50-d8_512x512_80k_ade20k_20200614_144016-f8ac5082.pth\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: python\n",
      "\tDescription: If you would like to only save the predicted results without evaluation as annotation is not released by the official dataset, you can set format_only=True and modify test_dataloader.\n",
      "As there is no annotation in dataset, we remove dict(type='LoadAnnotations') from test_dataloader Here is the example configuration:\n",
      "\tCode:\n",
      "test_evaluator = dict(\n",
      "    type='IoUMetric',\n",
      "    iou_metrics=['mIoU'],\n",
      "    format_only=True,\n",
      "    output_dir='work_dirs/format_results')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    num_workers=4,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
      "    dataset=dict(\n",
      "        type = 'ADE20KDataset'\n",
      "        data_root='data/ade/release_test',\n",
      "        data_prefix=dict(img_path='testing'),\n",
      "        # we don't load annotation in test transform pipeline.\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='Resize', scale=(2048, 512), keep_ratio=True),\n",
      "            dict(type='PackSegInputs')\n",
      "        ]))\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: then run test command:\n",
      "\tCode:\n",
      "python tools/test.py configs/fcn/fcn_r50-d8_4xb4-80k_ade20k-512x512.py ckpt/fcn_r50-d8_512x512_80k_ade20k_20200614_144016-f8ac5082.pth\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: python\n",
      "\tDescription: We recommend CityscapesMetric which is the wrapper of Cityscapes'sdk, when you want to\n",
      "save the predicted results of Cityscape test dataset to submit them in Cityscape test server. Here is the example configuration:\n",
      "\tCode:\n",
      "test_evaluator = dict(\n",
      "    type='CityscapesMetric',\n",
      "    format_only=True,\n",
      "    keep_results=True,\n",
      "    output_dir='work_dirs/format_results')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    num_workers=4,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
      "    dataset=dict(\n",
      "        type='CityscapesDataset',\n",
      "        data_root='data/cityscapes/',\n",
      "        data_prefix=dict(img_path='leftImg8bit/test'),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='Resize', scale=(2048, 1024), keep_ratio=True),\n",
      "            dict(type='PackSegInputs')\n",
      "        ]))\n",
      ")\n",
      "CodeBlock(\n",
      "\tLanguage: shell\n",
      "\tDescription: then run test command, for example:\n",
      "\tCode:\n",
      "python tools/test.py configs/fcn/fcn_r18-d8_4xb2-80k_cityscapes-512x1024.py ckpt/fcn_r18-d8_512x1024_80k_cityscapes_20201225_021327-6c50f8b4.pth\n",
      ")\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(repo.collections[90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rais",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
